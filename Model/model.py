# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WxDHqzntD8pMeuFAwogYLLtTpSo8cxwL
"""

import tensorflow as tf
tf.test.gpu_device_name()

# https://keras.io/
!pip install -q keras
import keras

# Import libraries and modules
import numpy as np
np.random.seed(123)  # for reproducibility
 
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, add, Input
from keras.layers.normalization import BatchNormalization
from keras.utils import np_utils
from keras.datasets import mnist

def residual_block(y):
    shortcut = y

    y = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)
    y = BatchNormalization()(y)
    y = Activation('relu')(y)

    y = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)
    y = BatchNormalization()(y)

    y = keras.layers.add([shortcut, y])
    y = Activation('relu')(y)

    return y

# 4. Load pre-shuffled MNIST data into train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()
 
# 5. Preprocess input data
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
 
# 6. Preprocess class labels
Y_train = np_utils.to_categorical(y_train, 10)
Y_test = np_utils.to_categorical(y_test, 10)

# define model architecture
model = Sequential()

# input convolution
model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) 
model.add(BatchNormalization())
model.add(Activation('relu'))

# residual blocks
model.add(Lambda(residual_block))
model.add(Lambda(residual_block))
model.add(Lambda(residual_block))
model.add(Lambda(residual_block))
model.add(Lambda(residual_block))

# output layer
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

# splitting model

inp = Input(shape=(28,28,1,))

# input convolution
y = Conv2D(32, (3, 3), input_shape=(28,28,1))(inp)
y = BatchNormalization()(y)
y = Activation('relu')(y)

# residual blocks
y = residual_block(y)
y = residual_block(y)
y = residual_block(y)
y = residual_block(y)
y = residual_block(y)

# policy
y_policy = Conv2D(2, (1, 1))(y)
y_policy = BatchNormalization()(y_policy)
y_policy = Flatten()(y_policy)
y_policy = Activation('relu')(y_policy)
policy_out = Dense(26, activation='softmax')(y_policy)

# value
y_value = Conv2D(1, (1, 1))(y)
y_value = BatchNormalization()(y_value)
y_value = Activation('relu')(y_value)
y_value = Dense(64, activation='relu')(y_value)
value_out = Dense(1, activation='tanh')(y_value)

model = Model(inp, [policy_out,value_out])

# next - custom loss function!

# 8. Compile model
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# 9. Fit model on training data
model.fit(X_train, Y_train, 
          batch_size=32, nb_epoch=10, verbose=1)
 
# 10. Evaluate model on test data
score = model.evaluate(X_test, Y_test, verbose=0)

print(score)

